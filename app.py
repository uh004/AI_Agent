# app.py
import os
import random
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import fitz  # PyMuPDF
from docx import Document
import gradio as gr

from typing import List, Dict, Literal
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END

# ----- í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ -----
from models.state_types import InterviewState
from models.schemas import ResumeAnalysis, QSItem, QSOutput, QSMultiOutput  # ì´ 3ê°œëŠ” models/schemas.pyì— ì •ì˜
from core.evaluator import evaluate_answer
from core.next_step import decide_next_step, change_strategy, route_next
from core.summarizer import summarize_interview
from core.generator import generate_question


# ========== íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ==========
def extract_text_from_file(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        doc = fitz.open(file_path)
        text = "\n".join(page.get_text() for page in doc)
        doc.close()
        return text
    elif ext == ".docx":
        doc = Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” DOCXë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")


# ========== LLM ==========
llm = ChatOpenAI(model="gpt-4.1-mini")


# ========== ì´ë ¥ì„œ ë¶„ì„ ==========
def analyze_resume(state: InterviewState) -> InterviewState:
    resume_text = state["resume_text"]

    prompt_template = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì¸ì‚¬ë‹´ë‹¹ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì´ë ¥ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ìš”ì•½ê³¼ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ë„ì¶œí•˜ì„¸ìš”. "
         "ê²°ê³¼ëŠ” JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”. "
         "1) summary: 3~5ë¬¸ì¥ ìš”ì•½  2) keywords: í•µì‹¬ ì—­ëŸ‰Â·ê¸°ìˆ Â·ì„±ê³¼Â·ê°•ì  ë¦¬ìŠ¤íŠ¸"),
        ("human", "ë¶„ì„í•  ì´ë ¥ì„œ í…ìŠ¤íŠ¸:\n---\n{resume_text}")
    ])

    chain = prompt_template | llm.with_structured_output(ResumeAnalysis)
    result: ResumeAnalysis = chain.invoke({"resume_text": resume_text})

    return {
        **state,
        "resume_summary": result.summary,
        "resume_keywords": result.keywords,
    }


# ========== ì§ˆë¬¸ ì „ëµ ìƒì„± ==========
def generate_question_strategy(state: InterviewState) -> InterviewState:
    summary = state.get("resume_summary", "")
    keywords = state.get("resume_keywords", [])

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì¸ì‚¬ë‹´ë‹¹ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.\n"
         "ì•„ë˜ ì´ë ¥ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ **3ëª…ì˜ ë©´ì ‘ê´€(A/B/C)**ì— ëŒ€í•´ ë©´ì ‘ ì§ˆë¬¸ ì „ëµì„ JSON ONLYë¡œ ìƒì„±í•˜ì„¸ìš”.\n\n"
         "ë©´ì ‘ê´€ ì—­í• :\n"
         "A = ì ì¬ë ¥(ë„ì „/ë¬¸ì œí•´ê²°/ì„±ì¥)\nB = ì¡°ì§ì í•©(í˜‘ì—…/ì†Œí†µ/ë¬¸í™”)\nC = ì§ë¬´ì—­ëŸ‰(ê¸°ìˆ /ì„±ê³¼)\n\n"
         "ê° ë©´ì ‘ê´€ì€ 3ê°œ í•­ëª©(ê²½í—˜/ë™ê¸°/ë…¼ë¦¬)ì— ëŒ€í•´\n"
         "- direction: í‰ê°€ ì˜ë„(1~2ë¬¸ì¥)\n"
         "- examples: êµ¬ì²´ì  ì˜ˆì‹œ ì§ˆë¬¸ 2~3ê°œ\n"),
        ("human",
         "ì´ë ¥ì„œ ìš”ì•½:\n{summary}\n\nì£¼ìš” í‚¤ì›Œë“œ:\n{keywords}\n\nJSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.")
    ])

    chain = prompt | llm.with_structured_output(QSMultiOutput)
    result: QSMultiOutput = chain.invoke({
        "summary": summary,
        "keywords": ", ".join(keywords) if isinstance(keywords, list) else str(keywords)
    })

    strategy_dict = {
        "ê²½í—˜": {
            "A": result.potential.experience.examples[0],
            "B": result.organization.experience.examples[0],
            "C": result.job.experience.examples[0],
        },
        "ë™ê¸°": {
            "A": result.potential.motivation.examples[0],
            "B": result.organization.motivation.examples[0],
            "C": result.job.motivation.examples[0],
        },
        "ë…¼ë¦¬": {
            "A": result.potential.logic.examples[0],
            "B": result.organization.logic.examples[0],
            "C": result.job.logic.examples[0],
        },
    }

    state["question_strategy"] = strategy_dict
    return state


# ========== ìƒíƒœ ì—…ë°ì´íŠ¸ ==========
def preProcessing_Interview(file_path: str) -> InterviewState:
    resume_text = extract_text_from_file(file_path)

    state: InterviewState = {
        "resume_text": resume_text,
        "resume_summary": "",
        "resume_keywords": [],
        "question_strategy": {},
        "current_question": "",
        "current_answer": "",
        "current_strategy": "",
        "conversation": [],
        "evaluation": [],
        "next_step": "",
        "deep_counts": {}
    }

    state = analyze_resume(state)
    state = generate_question_strategy(state)

    # ì²« ì§ˆë¬¸ ì„ íƒ (ì¹´í…Œê³ ë¦¬ ê³ ì • + ë©´ì ‘ê´€ ëœë¤)
    categories = ["ê²½í—˜", "ë™ê¸°", "ë…¼ë¦¬"]
    interviewers = ["A", "B", "C"]
    cat = categories[0]
    iv = random.choice(interviewers)
    selected_question = state["question_strategy"][cat][iv]

    state["current_question"] = selected_question
    state["current_strategy"] = cat
    return state


def update_current_answer(state: InterviewState, user_answer: str) -> InterviewState:
    return {**state, "current_answer": user_answer.strip()}


# ========== LangGraph êµ¬ì„± ==========
def _update_answer_node(state: InterviewState) -> InterviewState:
    # Gradioì—ì„œ ì „ë‹¬ëœ ë‹µë³€ì´ state["current_answer"]ì— ì´ë¯¸ ë“¤ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ í†µê³¼
    return state


builder = StateGraph(InterviewState)
builder.add_node("update_answer", _update_answer_node)
builder.add_node("evaluate", evaluate_answer)
builder.add_node("decide", decide_next_step)
builder.add_node("generate", generate_question)
builder.add_node("change_strategy", change_strategy)
builder.add_node("summarize", summarize_interview)

builder.set_entry_point("update_answer")
builder.add_edge("update_answer", "evaluate")
builder.add_edge("evaluate", "decide")
builder.add_conditional_edges(
    "decide",
    route_next,
    {"generate": "generate", "change_strategy": "change_strategy", "summarize": "summarize"}
)
builder.add_edge("generate", END)
builder.add_edge("change_strategy", END)
builder.add_edge("summarize", END)

graph = builder.compile()


# ========== Gradio UI ==========
def initialize_state():
    return {
        "state": None,
        "interview_started": False,
        "interview_ended": False,
        "chat_history": []  # List[Tuple[str|None, str|None]]
    }


def upload_and_initialize(file_obj, session_state):
    if file_obj is None:
        return session_state, [(None, "íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")]

    file_path = file_obj.name
    state = preProcessing_Interview(file_path)
    session_state["state"] = state
    session_state["interview_started"] = True
    session_state["interview_ended"] = False
    session_state["chat_history"] = [(None, state["current_question"])]
    return session_state, session_state["chat_history"]


def chat_interview(user_input, session_state):
    # ì´ë¯¸ ì¢…ë£Œëœ ì„¸ì…˜ì´ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥ë§Œ ìœ ì§€
    if session_state.get("interview_ended"):
        return session_state, session_state["chat_history"], gr.update(value="")

    if not session_state.get("interview_started"):
        return session_state, [(None, "ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸í„°ë·°ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")], gr.update(value="")

    # ì‚¬ìš©ì ë°œí™” ì¶”ê°€ (íŠœí”Œ: (user, bot))
    session_state["chat_history"].append((user_input, None))

    # ìƒíƒœì— ë‹µë³€ ë°˜ì˜ â†’ ê·¸ë˜í”„ 1íšŒ ì‹¤í–‰
    session_state["state"] = update_current_answer(session_state["state"], user_input)
    session_state["state"] = graph.invoke(session_state["state"])

    # ì¢…ë£Œ ë¶„ê¸°
    if session_state["state"]["next_step"] == "end":
        session_state["interview_ended"] = True

        # summarize_interviewê°€ ì´ë¯¸ stateë¥¼ endë¡œ ë°”ê¿¨ìœ¼ë¯€ë¡œ, ìµœì¢… ë³´ê³ ì„œë§Œ ë©”ì‹œì§€ë¡œ ì¶”ê°€
        # (í•„ìš” ì‹œ stateì˜ conversation/evaluationì„ í¬ë§·íŒ…í•´ ìš”ì•½ ë¬¸ìì—´ êµ¬ì„± ê°€ëŠ¥)
        final_msg = "âœ… ì¸í„°ë·°ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¢Œì¸¡ ì½˜ì†” ì¶œë ¥(í˜¹ì€ ì„œë²„ ë¡œê·¸)ì˜ í”¼ë“œë°± ë³´ê³ ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."
        session_state["chat_history"].append((None, final_msg))
        return session_state, session_state["chat_history"], gr.update(value="")

    # ë‹¤ìŒ ì§ˆë¬¸ ì§„í–‰
    next_question = session_state["state"]["current_question"]
    session_state["chat_history"].append((None, next_question))
    return session_state, session_state["chat_history"], gr.update(value="")


with gr.Blocks() as demo:
    session_state = gr.State(initialize_state())

    gr.Markdown("# ğŸ¤– AI ë©´ì ‘ê´€\nì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸í„°ë·°ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")

    with gr.Row():
        file_input = gr.File(label="ì´ë ¥ì„œ ì—…ë¡œë“œ (PDF ë˜ëŠ” DOCX)")
        upload_btn = gr.Button("ì¸í„°ë·° ì‹œì‘")

    chatbot = gr.Chatbot(label="AI ë©´ì ‘ê´€", height=480)  # íŠœí”Œ ê¸°ë°˜
    user_input = gr.Textbox(show_label=False, placeholder="ë‹µë³€ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")

    upload_btn.click(upload_and_initialize,
                     inputs=[file_input, session_state],
                     outputs=[session_state, chatbot])

    user_input.submit(chat_interview,
                      inputs=[user_input, session_state],
                      outputs=[session_state, chatbot, user_input])

    # ì…ë ¥ì°½ ìë™ ë¹„ìš°ê¸°
    user_input.submit(lambda: "", None, user_input)

# Colab/ì„œë²„ ëª¨ë‘ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ queue + ì™¸ë¶€ ë§í¬ ê¶Œì¥
demo.queue().launch(share=True, inline=False)
